{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PanDerm - Skin Lesion Segmentation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from PIL import Image\n",
    "from models.cae_seg import CAEv2_seg\n",
    "from utils.train_utils import largestConnectComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "save_path = './'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# load dataset\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "image = cv2.imread('/data2/wangzh/datasets/ISIC2018/Test_Data/ISIC_0012236.jpg')[..., ::-1]\n",
    "image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "image = Image.fromarray(np.uint8(image))\n",
    "image = image_transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CAEv2_seg()\n",
    "\n",
    "# load model weights\n",
    "model_path = '/data/wangzh/experiments/skinfm/finals/cae_seg_isic18/lr_1e-4_decay_0.05_full/0/model_best_0.ckpt'\n",
    "pretrained_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "pretrained_dict = pretrained_dict[\"state_dict\"]\n",
    "model_dict = model.state_dict()\n",
    "print('Model dict: ', model_dict.keys())\n",
    "available_pretrained_dict = {}\n",
    "\n",
    "for k, v in pretrained_dict.items():\n",
    "    print('Pretrained dict: ', k)\n",
    "    if k in model_dict.keys():\n",
    "        if pretrained_dict[k].shape == model_dict[k].shape:\n",
    "            available_pretrained_dict[k] = v\n",
    "    if k[6:] in model_dict.keys():\n",
    "        if pretrained_dict[k].shape == model_dict[k[6:]].shape:\n",
    "            available_pretrained_dict[k[6:]] = v\n",
    "\n",
    "for k, _ in available_pretrained_dict.items():\n",
    "    print(\"loading {}\".format(k))\n",
    "model_dict.update(available_pretrained_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "image_save = image.squeeze().cpu().detach().numpy()\n",
    "image_save = image_save * 0.5 + 0.5\n",
    "image_save = np.transpose(image_save, (1, 2, 0))\n",
    "\n",
    "output = torch.argmax(output.squeeze(), dim=0).cpu().detach().numpy()\n",
    "output = largestConnectComponent(output)\n",
    "\n",
    "output = mark_boundaries(image_save, output, color=(0, 1, 1), mode='thick')\n",
    "output = (output * 255).astype(np.uint8)\n",
    "\n",
    "cv2.imwrite(os.path.join(save_path, 'result.png'), output[..., ::-1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
